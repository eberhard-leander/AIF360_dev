{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates the use of the fairness adjuster, using the same structure as the AIF360 adversarial debiasing example\n",
    "\n",
    "The source notebook can be found here:\n",
    "https://github.com/Trusted-AI/AIF360/blob/main/examples/demo_adversarial_debiasing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 18:49:48.784833: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-04 18:49:48.786743: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-04 18:49:48.813915: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 18:49:48.813940: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 18:49:48.813959: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 18:49:48.819230: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-04 18:49:48.819989: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 18:49:49.376410: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult, load_preproc_data_compas, load_preproc_data_german\n",
    "\n",
    "from aif360.algorithms.inprocessing.fairness_adjuster import FairnessAdjuster\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "dataset_orig = load_preproc_data_adult()\n",
    "\n",
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34189, 18)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex', 'race']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.])] [array([0.]), array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, \n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric for original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.197096\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.188498\n"
     ]
    }
   ],
   "source": [
    "# Metric for the original dataset\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "metric_orig_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Scaled dataset - Verify that the scaling does not affect the group label statistics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.197096\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.188498\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MaxAbsScaler()\n",
    "dataset_orig_train.features = min_max_scaler.fit_transform(dataset_orig_train.features)\n",
    "dataset_orig_test.features = min_max_scaler.transform(dataset_orig_test.features)\n",
    "metric_scaled_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Scaled dataset - Verify that the scaling does not affect the group label statistics\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_train.mean_difference())\n",
    "metric_scaled_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_test.mean_difference())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn plan classifier without debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load post-processing algorithm that equalizes the odds\n",
    "# Learn parameters with debias set to False\n",
    "sess = tf.Session()\n",
    "plain_model = FairnessAdjuster(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='plain_classifier',\n",
    "                          debias=False,\n",
    "                          sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/aif360/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "epoch 0; iter: 0; batch classifier loss: 0.730466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 18:49:51.832598: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 200; batch classifier loss: 0.384945\n",
      "epoch 1; iter: 0; batch classifier loss: 0.449795\n",
      "epoch 1; iter: 200; batch classifier loss: 0.362988\n",
      "epoch 2; iter: 0; batch classifier loss: 0.484586\n",
      "epoch 2; iter: 200; batch classifier loss: 0.421734\n",
      "epoch 3; iter: 0; batch classifier loss: 0.453353\n",
      "epoch 3; iter: 200; batch classifier loss: 0.468006\n",
      "epoch 4; iter: 0; batch classifier loss: 0.339172\n",
      "epoch 4; iter: 200; batch classifier loss: 0.472340\n",
      "epoch 5; iter: 0; batch classifier loss: 0.415168\n",
      "epoch 5; iter: 200; batch classifier loss: 0.428082\n",
      "epoch 6; iter: 0; batch classifier loss: 0.475164\n",
      "epoch 6; iter: 200; batch classifier loss: 0.408093\n",
      "epoch 7; iter: 0; batch classifier loss: 0.452335\n",
      "epoch 7; iter: 200; batch classifier loss: 0.424391\n",
      "epoch 8; iter: 0; batch classifier loss: 0.473423\n",
      "epoch 8; iter: 200; batch classifier loss: 0.330901\n",
      "epoch 9; iter: 0; batch classifier loss: 0.399846\n",
      "epoch 9; iter: 200; batch classifier loss: 0.396913\n",
      "epoch 10; iter: 0; batch classifier loss: 0.413767\n",
      "epoch 10; iter: 200; batch classifier loss: 0.402544\n",
      "epoch 11; iter: 0; batch classifier loss: 0.332542\n",
      "epoch 11; iter: 200; batch classifier loss: 0.451443\n",
      "epoch 12; iter: 0; batch classifier loss: 0.391172\n",
      "epoch 12; iter: 200; batch classifier loss: 0.361837\n",
      "epoch 13; iter: 0; batch classifier loss: 0.509326\n",
      "epoch 13; iter: 200; batch classifier loss: 0.439083\n",
      "epoch 14; iter: 0; batch classifier loss: 0.462151\n",
      "epoch 14; iter: 200; batch classifier loss: 0.341866\n",
      "epoch 15; iter: 0; batch classifier loss: 0.360938\n",
      "epoch 15; iter: 200; batch classifier loss: 0.392112\n",
      "epoch 16; iter: 0; batch classifier loss: 0.440434\n",
      "epoch 16; iter: 200; batch classifier loss: 0.515570\n",
      "epoch 17; iter: 0; batch classifier loss: 0.415105\n",
      "epoch 17; iter: 200; batch classifier loss: 0.407445\n",
      "epoch 18; iter: 0; batch classifier loss: 0.388465\n",
      "epoch 18; iter: 200; batch classifier loss: 0.347401\n",
      "epoch 19; iter: 0; batch classifier loss: 0.407677\n",
      "epoch 19; iter: 200; batch classifier loss: 0.419564\n",
      "epoch 20; iter: 0; batch classifier loss: 0.401245\n",
      "epoch 20; iter: 200; batch classifier loss: 0.403454\n",
      "epoch 21; iter: 0; batch classifier loss: 0.407800\n",
      "epoch 21; iter: 200; batch classifier loss: 0.480004\n",
      "epoch 22; iter: 0; batch classifier loss: 0.493223\n",
      "epoch 22; iter: 200; batch classifier loss: 0.445361\n",
      "epoch 23; iter: 0; batch classifier loss: 0.414039\n",
      "epoch 23; iter: 200; batch classifier loss: 0.418270\n",
      "epoch 24; iter: 0; batch classifier loss: 0.477350\n",
      "epoch 24; iter: 200; batch classifier loss: 0.350315\n",
      "epoch 25; iter: 0; batch classifier loss: 0.396251\n",
      "epoch 25; iter: 200; batch classifier loss: 0.451328\n",
      "epoch 26; iter: 0; batch classifier loss: 0.395468\n",
      "epoch 26; iter: 200; batch classifier loss: 0.341646\n",
      "epoch 27; iter: 0; batch classifier loss: 0.443462\n",
      "epoch 27; iter: 200; batch classifier loss: 0.458426\n",
      "epoch 28; iter: 0; batch classifier loss: 0.351647\n",
      "epoch 28; iter: 200; batch classifier loss: 0.427028\n",
      "epoch 29; iter: 0; batch classifier loss: 0.312486\n",
      "epoch 29; iter: 200; batch classifier loss: 0.433873\n",
      "epoch 30; iter: 0; batch classifier loss: 0.402280\n",
      "epoch 30; iter: 200; batch classifier loss: 0.386222\n",
      "epoch 31; iter: 0; batch classifier loss: 0.347466\n",
      "epoch 31; iter: 200; batch classifier loss: 0.453163\n",
      "epoch 32; iter: 0; batch classifier loss: 0.471735\n",
      "epoch 32; iter: 200; batch classifier loss: 0.414090\n",
      "epoch 33; iter: 0; batch classifier loss: 0.352315\n",
      "epoch 33; iter: 200; batch classifier loss: 0.510469\n",
      "epoch 34; iter: 0; batch classifier loss: 0.454725\n",
      "epoch 34; iter: 200; batch classifier loss: 0.453485\n",
      "epoch 35; iter: 0; batch classifier loss: 0.450350\n",
      "epoch 35; iter: 200; batch classifier loss: 0.430188\n",
      "epoch 36; iter: 0; batch classifier loss: 0.463375\n",
      "epoch 36; iter: 200; batch classifier loss: 0.368432\n",
      "epoch 37; iter: 0; batch classifier loss: 0.468164\n",
      "epoch 37; iter: 200; batch classifier loss: 0.495129\n",
      "epoch 38; iter: 0; batch classifier loss: 0.382451\n",
      "epoch 38; iter: 200; batch classifier loss: 0.360185\n",
      "epoch 39; iter: 0; batch classifier loss: 0.470767\n",
      "epoch 39; iter: 200; batch classifier loss: 0.425332\n",
      "epoch 40; iter: 0; batch classifier loss: 0.488802\n",
      "epoch 40; iter: 200; batch classifier loss: 0.468464\n",
      "epoch 41; iter: 0; batch classifier loss: 0.403350\n",
      "epoch 41; iter: 200; batch classifier loss: 0.348372\n",
      "epoch 42; iter: 0; batch classifier loss: 0.466748\n",
      "epoch 42; iter: 200; batch classifier loss: 0.440284\n",
      "epoch 43; iter: 0; batch classifier loss: 0.424354\n",
      "epoch 43; iter: 200; batch classifier loss: 0.477739\n",
      "epoch 44; iter: 0; batch classifier loss: 0.437055\n",
      "epoch 44; iter: 200; batch classifier loss: 0.429645\n",
      "epoch 45; iter: 0; batch classifier loss: 0.390903\n",
      "epoch 45; iter: 200; batch classifier loss: 0.426537\n",
      "epoch 46; iter: 0; batch classifier loss: 0.385720\n",
      "epoch 46; iter: 200; batch classifier loss: 0.480805\n",
      "epoch 47; iter: 0; batch classifier loss: 0.345654\n",
      "epoch 47; iter: 200; batch classifier loss: 0.389200\n",
      "epoch 48; iter: 0; batch classifier loss: 0.355367\n",
      "epoch 48; iter: 200; batch classifier loss: 0.337305\n",
      "epoch 49; iter: 0; batch classifier loss: 0.394207\n",
      "epoch 49; iter: 200; batch classifier loss: 0.410290\n",
      "epoch 0; iter: 0; batch adjuster loss: 0.032253\n",
      "epoch 0; iter: 200; batch adjuster loss: 0.001813\n",
      "epoch 1; iter: 0; batch adjuster loss: 0.001169\n",
      "epoch 1; iter: 200; batch adjuster loss: 0.000315\n",
      "epoch 2; iter: 0; batch adjuster loss: 0.000265\n",
      "epoch 2; iter: 200; batch adjuster loss: 0.000161\n",
      "epoch 3; iter: 0; batch adjuster loss: 0.000093\n",
      "epoch 3; iter: 200; batch adjuster loss: 0.000047\n",
      "epoch 4; iter: 0; batch adjuster loss: 0.000033\n",
      "epoch 4; iter: 200; batch adjuster loss: 0.000031\n",
      "epoch 5; iter: 0; batch adjuster loss: 0.000018\n",
      "epoch 5; iter: 200; batch adjuster loss: 0.000020\n",
      "epoch 6; iter: 0; batch adjuster loss: 0.000013\n",
      "epoch 6; iter: 200; batch adjuster loss: 0.000012\n",
      "epoch 7; iter: 0; batch adjuster loss: 0.000015\n",
      "epoch 7; iter: 200; batch adjuster loss: 0.000006\n",
      "epoch 8; iter: 0; batch adjuster loss: 0.000011\n",
      "epoch 8; iter: 200; batch adjuster loss: 0.000006\n",
      "epoch 9; iter: 0; batch adjuster loss: 0.000007\n",
      "epoch 9; iter: 200; batch adjuster loss: 0.000003\n",
      "epoch 10; iter: 0; batch adjuster loss: 0.000004\n",
      "epoch 10; iter: 200; batch adjuster loss: 0.000002\n",
      "epoch 11; iter: 0; batch adjuster loss: 0.000003\n",
      "epoch 11; iter: 200; batch adjuster loss: 0.000002\n",
      "epoch 12; iter: 0; batch adjuster loss: 0.000002\n",
      "epoch 12; iter: 200; batch adjuster loss: 0.000001\n",
      "epoch 13; iter: 0; batch adjuster loss: 0.000001\n",
      "epoch 13; iter: 200; batch adjuster loss: 0.000001\n",
      "epoch 14; iter: 0; batch adjuster loss: 0.000001\n",
      "epoch 14; iter: 200; batch adjuster loss: 0.000001\n",
      "epoch 15; iter: 0; batch adjuster loss: 0.000001\n",
      "epoch 15; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 16; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 16; iter: 200; batch adjuster loss: 0.000001\n",
      "epoch 17; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 17; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 18; iter: 0; batch adjuster loss: 0.000001\n",
      "epoch 18; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 19; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 19; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 20; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 20; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 21; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 21; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 22; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 22; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 23; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 23; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 24; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 24; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 25; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 25; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 26; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 26; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 27; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 27; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 28; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 28; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 29; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 29; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 30; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 30; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 31; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 31; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 32; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 32; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 33; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 33; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 34; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 34; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 35; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 35; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 36; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 36; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 37; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 37; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 38; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 38; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 39; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 39; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 40; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 40; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 41; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 41; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 42; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 42; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 43; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 43; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 44; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 44; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 45; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 45; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 46; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 46; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 47; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 47; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 48; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 48; iter: 200; batch adjuster loss: 0.000000\n",
      "epoch 49; iter: 0; batch adjuster loss: 0.000000\n",
      "epoch 49; iter: 200; batch adjuster loss: 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.fairness_adjuster.FairnessAdjuster at 0x7fb21cdba0e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 18:50:10.664675: W tensorflow/c/c_api.cc:305] Operation '{name:'plain_classifier/plain_classifier/classifier_model/b2/Adam_1/Assign' id:268 op device:{requested: '', assigned: ''} def:{{{node plain_classifier/plain_classifier/classifier_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](plain_classifier/plain_classifier/classifier_model/b2/Adam_1, plain_classifier/plain_classifier/classifier_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_nodebiasing_train = plain_model.predict(dataset_orig_train)\n",
    "dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.243550\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.238385\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.804067\n",
      "Test set: Balanced classification accuracy = 0.673824\n",
      "Test set: Disparate impact = 0.000000\n",
      "Test set: Equal opportunity difference = -0.502031\n",
      "Test set: Average odds difference = -0.313805\n",
      "Test set: Theil_index = 0.169696\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "metric_dataset_nodebiasing_train = BinaryLabelDatasetMetric(dataset_nodebiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_nodebiasing_test = BinaryLabelDatasetMetric(dataset_nodebiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "classified_metric_nodebiasing_test = ClassificationMetric(dataset_orig_test, \n",
    "                                                 dataset_nodebiasing_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply in-processing algorithm based on adversarial learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn parameters with debias set to True\n",
    "debiased_model = FairnessAdjuster(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          adversary_loss_weight=0.01,\n",
    "                          debias=True,\n",
    "                          sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.699307\n",
      "epoch 0; iter: 200; batch classifier loss: 0.410342\n",
      "epoch 1; iter: 0; batch classifier loss: 0.351379\n",
      "epoch 1; iter: 200; batch classifier loss: 0.459765\n",
      "epoch 2; iter: 0; batch classifier loss: 0.348140\n",
      "epoch 2; iter: 200; batch classifier loss: 0.368497\n",
      "epoch 3; iter: 0; batch classifier loss: 0.385407\n",
      "epoch 3; iter: 200; batch classifier loss: 0.366745\n",
      "epoch 4; iter: 0; batch classifier loss: 0.469812\n",
      "epoch 4; iter: 200; batch classifier loss: 0.471754\n",
      "epoch 5; iter: 0; batch classifier loss: 0.427997\n",
      "epoch 5; iter: 200; batch classifier loss: 0.374578\n",
      "epoch 6; iter: 0; batch classifier loss: 0.444535\n",
      "epoch 6; iter: 200; batch classifier loss: 0.438828\n",
      "epoch 7; iter: 0; batch classifier loss: 0.491524\n",
      "epoch 7; iter: 200; batch classifier loss: 0.361721\n",
      "epoch 8; iter: 0; batch classifier loss: 0.423452\n",
      "epoch 8; iter: 200; batch classifier loss: 0.449372\n",
      "epoch 9; iter: 0; batch classifier loss: 0.377840\n",
      "epoch 9; iter: 200; batch classifier loss: 0.395748\n",
      "epoch 10; iter: 0; batch classifier loss: 0.433790\n",
      "epoch 10; iter: 200; batch classifier loss: 0.413357\n",
      "epoch 11; iter: 0; batch classifier loss: 0.397204\n",
      "epoch 11; iter: 200; batch classifier loss: 0.371545\n",
      "epoch 12; iter: 0; batch classifier loss: 0.489183\n",
      "epoch 12; iter: 200; batch classifier loss: 0.435414\n",
      "epoch 13; iter: 0; batch classifier loss: 0.473785\n",
      "epoch 13; iter: 200; batch classifier loss: 0.432624\n",
      "epoch 14; iter: 0; batch classifier loss: 0.430936\n",
      "epoch 14; iter: 200; batch classifier loss: 0.339535\n",
      "epoch 15; iter: 0; batch classifier loss: 0.482920\n",
      "epoch 15; iter: 200; batch classifier loss: 0.413189\n",
      "epoch 16; iter: 0; batch classifier loss: 0.439174\n",
      "epoch 16; iter: 200; batch classifier loss: 0.506259\n",
      "epoch 17; iter: 0; batch classifier loss: 0.428569\n",
      "epoch 17; iter: 200; batch classifier loss: 0.415149\n",
      "epoch 18; iter: 0; batch classifier loss: 0.349955\n",
      "epoch 18; iter: 200; batch classifier loss: 0.359166\n",
      "epoch 19; iter: 0; batch classifier loss: 0.397061\n",
      "epoch 19; iter: 200; batch classifier loss: 0.376896\n",
      "epoch 20; iter: 0; batch classifier loss: 0.411169\n",
      "epoch 20; iter: 200; batch classifier loss: 0.431428\n",
      "epoch 21; iter: 0; batch classifier loss: 0.283416\n",
      "epoch 21; iter: 200; batch classifier loss: 0.480711\n",
      "epoch 22; iter: 0; batch classifier loss: 0.360186\n",
      "epoch 22; iter: 200; batch classifier loss: 0.384753\n",
      "epoch 23; iter: 0; batch classifier loss: 0.469986\n",
      "epoch 23; iter: 200; batch classifier loss: 0.387432\n",
      "epoch 24; iter: 0; batch classifier loss: 0.472557\n",
      "epoch 24; iter: 200; batch classifier loss: 0.390869\n",
      "epoch 25; iter: 0; batch classifier loss: 0.443398\n",
      "epoch 25; iter: 200; batch classifier loss: 0.464419\n",
      "epoch 26; iter: 0; batch classifier loss: 0.421519\n",
      "epoch 26; iter: 200; batch classifier loss: 0.375465\n",
      "epoch 27; iter: 0; batch classifier loss: 0.452397\n",
      "epoch 27; iter: 200; batch classifier loss: 0.317375\n",
      "epoch 28; iter: 0; batch classifier loss: 0.410560\n",
      "epoch 28; iter: 200; batch classifier loss: 0.427612\n",
      "epoch 29; iter: 0; batch classifier loss: 0.444698\n",
      "epoch 29; iter: 200; batch classifier loss: 0.401386\n",
      "epoch 30; iter: 0; batch classifier loss: 0.465044\n",
      "epoch 30; iter: 200; batch classifier loss: 0.384919\n",
      "epoch 31; iter: 0; batch classifier loss: 0.402338\n",
      "epoch 31; iter: 200; batch classifier loss: 0.423976\n",
      "epoch 32; iter: 0; batch classifier loss: 0.402201\n",
      "epoch 32; iter: 200; batch classifier loss: 0.451070\n",
      "epoch 33; iter: 0; batch classifier loss: 0.454529\n",
      "epoch 33; iter: 200; batch classifier loss: 0.542670\n",
      "epoch 34; iter: 0; batch classifier loss: 0.346407\n",
      "epoch 34; iter: 200; batch classifier loss: 0.425164\n",
      "epoch 35; iter: 0; batch classifier loss: 0.339745\n",
      "epoch 35; iter: 200; batch classifier loss: 0.485415\n",
      "epoch 36; iter: 0; batch classifier loss: 0.406322\n",
      "epoch 36; iter: 200; batch classifier loss: 0.485374\n",
      "epoch 37; iter: 0; batch classifier loss: 0.538189\n",
      "epoch 37; iter: 200; batch classifier loss: 0.363010\n",
      "epoch 38; iter: 0; batch classifier loss: 0.322446\n",
      "epoch 38; iter: 200; batch classifier loss: 0.286836\n",
      "epoch 39; iter: 0; batch classifier loss: 0.487763\n",
      "epoch 39; iter: 200; batch classifier loss: 0.388439\n",
      "epoch 40; iter: 0; batch classifier loss: 0.391467\n",
      "epoch 40; iter: 200; batch classifier loss: 0.432563\n",
      "epoch 41; iter: 0; batch classifier loss: 0.382473\n",
      "epoch 41; iter: 200; batch classifier loss: 0.462766\n",
      "epoch 42; iter: 0; batch classifier loss: 0.433792\n",
      "epoch 42; iter: 200; batch classifier loss: 0.384333\n",
      "epoch 43; iter: 0; batch classifier loss: 0.487081\n",
      "epoch 43; iter: 200; batch classifier loss: 0.378676\n",
      "epoch 44; iter: 0; batch classifier loss: 0.381372\n",
      "epoch 44; iter: 200; batch classifier loss: 0.367106\n",
      "epoch 45; iter: 0; batch classifier loss: 0.362695\n",
      "epoch 45; iter: 200; batch classifier loss: 0.400130\n",
      "epoch 46; iter: 0; batch classifier loss: 0.402627\n",
      "epoch 46; iter: 200; batch classifier loss: 0.363627\n",
      "epoch 47; iter: 0; batch classifier loss: 0.414636\n",
      "epoch 47; iter: 200; batch classifier loss: 0.440171\n",
      "epoch 48; iter: 0; batch classifier loss: 0.397285\n",
      "epoch 48; iter: 200; batch classifier loss: 0.388308\n",
      "epoch 49; iter: 0; batch classifier loss: 0.469326\n",
      "epoch 49; iter: 200; batch classifier loss: 0.531798\n",
      "epoch 0; iter: 0; batch adjuster loss: 0.036688; batch classifier loss; 0.446666; batch adversarial loss: 0.763236\n",
      "epoch 0; iter: 200; batch adjuster loss: 0.005746; batch classifier loss; 0.361303; batch adversarial loss: 0.695625\n",
      "epoch 1; iter: 0; batch adjuster loss: 0.002005; batch classifier loss; 0.462450; batch adversarial loss: 0.672632\n",
      "epoch 1; iter: 200; batch adjuster loss: 0.000572; batch classifier loss; 0.413985; batch adversarial loss: 0.643250\n",
      "epoch 2; iter: 0; batch adjuster loss: 0.000525; batch classifier loss; 0.386807; batch adversarial loss: 0.649508\n",
      "epoch 2; iter: 200; batch adjuster loss: 0.000397; batch classifier loss; 0.361690; batch adversarial loss: 0.678594\n",
      "epoch 3; iter: 0; batch adjuster loss: 0.000481; batch classifier loss; 0.386794; batch adversarial loss: 0.613227\n",
      "epoch 3; iter: 200; batch adjuster loss: 0.001087; batch classifier loss; 0.317380; batch adversarial loss: 0.586736\n",
      "epoch 4; iter: 0; batch adjuster loss: 0.001504; batch classifier loss; 0.447537; batch adversarial loss: 0.595895\n",
      "epoch 4; iter: 200; batch adjuster loss: 0.003324; batch classifier loss; 0.388623; batch adversarial loss: 0.585152\n",
      "epoch 5; iter: 0; batch adjuster loss: 0.004809; batch classifier loss; 0.373168; batch adversarial loss: 0.612595\n",
      "epoch 5; iter: 200; batch adjuster loss: 0.014539; batch classifier loss; 0.469422; batch adversarial loss: 0.608306\n",
      "epoch 6; iter: 0; batch adjuster loss: 0.013090; batch classifier loss; 0.413621; batch adversarial loss: 0.562861\n",
      "epoch 6; iter: 200; batch adjuster loss: 0.020356; batch classifier loss; 0.418543; batch adversarial loss: 0.574758\n",
      "epoch 7; iter: 0; batch adjuster loss: 0.039468; batch classifier loss; 0.375957; batch adversarial loss: 0.594182\n",
      "epoch 7; iter: 200; batch adjuster loss: 0.032594; batch classifier loss; 0.321550; batch adversarial loss: 0.609077\n",
      "epoch 8; iter: 0; batch adjuster loss: 0.063227; batch classifier loss; 0.435045; batch adversarial loss: 0.570062\n",
      "epoch 8; iter: 200; batch adjuster loss: 0.144053; batch classifier loss; 0.465441; batch adversarial loss: 0.594165\n",
      "epoch 9; iter: 0; batch adjuster loss: 0.211248; batch classifier loss; 0.449433; batch adversarial loss: 0.674439\n",
      "epoch 9; iter: 200; batch adjuster loss: 0.143762; batch classifier loss; 0.443326; batch adversarial loss: 0.567898\n",
      "epoch 10; iter: 0; batch adjuster loss: 0.146917; batch classifier loss; 0.339586; batch adversarial loss: 0.647734\n",
      "epoch 10; iter: 200; batch adjuster loss: 0.121687; batch classifier loss; 0.476388; batch adversarial loss: 0.568904\n",
      "epoch 11; iter: 0; batch adjuster loss: 0.117737; batch classifier loss; 0.357094; batch adversarial loss: 0.639179\n",
      "epoch 11; iter: 200; batch adjuster loss: 0.044024; batch classifier loss; 0.434411; batch adversarial loss: 0.608890\n",
      "epoch 12; iter: 0; batch adjuster loss: 0.026894; batch classifier loss; 0.468217; batch adversarial loss: 0.593719\n",
      "epoch 12; iter: 200; batch adjuster loss: 0.028196; batch classifier loss; 0.485310; batch adversarial loss: 0.550074\n",
      "epoch 13; iter: 0; batch adjuster loss: 0.033597; batch classifier loss; 0.412482; batch adversarial loss: 0.602520\n",
      "epoch 13; iter: 200; batch adjuster loss: 0.047568; batch classifier loss; 0.517170; batch adversarial loss: 0.637392\n",
      "epoch 14; iter: 0; batch adjuster loss: 0.026234; batch classifier loss; 0.314223; batch adversarial loss: 0.550014\n",
      "epoch 14; iter: 200; batch adjuster loss: 0.037035; batch classifier loss; 0.435517; batch adversarial loss: 0.636145\n",
      "epoch 15; iter: 0; batch adjuster loss: 0.033289; batch classifier loss; 0.415669; batch adversarial loss: 0.597921\n",
      "epoch 15; iter: 200; batch adjuster loss: 0.034270; batch classifier loss; 0.368005; batch adversarial loss: 0.558094\n",
      "epoch 16; iter: 0; batch adjuster loss: 0.041395; batch classifier loss; 0.444429; batch adversarial loss: 0.614052\n",
      "epoch 16; iter: 200; batch adjuster loss: 0.037538; batch classifier loss; 0.473160; batch adversarial loss: 0.588638\n",
      "epoch 17; iter: 0; batch adjuster loss: 0.032260; batch classifier loss; 0.385174; batch adversarial loss: 0.580020\n",
      "epoch 17; iter: 200; batch adjuster loss: 0.044758; batch classifier loss; 0.421764; batch adversarial loss: 0.555010\n",
      "epoch 18; iter: 0; batch adjuster loss: 0.036029; batch classifier loss; 0.386893; batch adversarial loss: 0.568720\n",
      "epoch 18; iter: 200; batch adjuster loss: 0.124103; batch classifier loss; 0.457110; batch adversarial loss: 0.613203\n",
      "epoch 19; iter: 0; batch adjuster loss: 0.105822; batch classifier loss; 0.422667; batch adversarial loss: 0.692868\n",
      "epoch 19; iter: 200; batch adjuster loss: 0.099121; batch classifier loss; 0.482181; batch adversarial loss: 0.634845\n",
      "epoch 20; iter: 0; batch adjuster loss: 0.072729; batch classifier loss; 0.421334; batch adversarial loss: 0.521057\n",
      "epoch 20; iter: 200; batch adjuster loss: 0.057223; batch classifier loss; 0.463425; batch adversarial loss: 0.570008\n",
      "epoch 21; iter: 0; batch adjuster loss: 0.047531; batch classifier loss; 0.420535; batch adversarial loss: 0.594252\n",
      "epoch 21; iter: 200; batch adjuster loss: 0.095648; batch classifier loss; 0.382568; batch adversarial loss: 0.619018\n",
      "epoch 22; iter: 0; batch adjuster loss: 0.050561; batch classifier loss; 0.404167; batch adversarial loss: 0.605926\n",
      "epoch 22; iter: 200; batch adjuster loss: 0.106595; batch classifier loss; 0.380055; batch adversarial loss: 0.604136\n",
      "epoch 23; iter: 0; batch adjuster loss: 0.121405; batch classifier loss; 0.357352; batch adversarial loss: 0.650039\n",
      "epoch 23; iter: 200; batch adjuster loss: 0.107828; batch classifier loss; 0.422197; batch adversarial loss: 0.651574\n",
      "epoch 24; iter: 0; batch adjuster loss: 0.075664; batch classifier loss; 0.456069; batch adversarial loss: 0.537609\n",
      "epoch 24; iter: 200; batch adjuster loss: 0.079711; batch classifier loss; 0.485041; batch adversarial loss: 0.521779\n",
      "epoch 25; iter: 0; batch adjuster loss: 0.106539; batch classifier loss; 0.425853; batch adversarial loss: 0.684870\n",
      "epoch 25; iter: 200; batch adjuster loss: 0.079040; batch classifier loss; 0.415302; batch adversarial loss: 0.588288\n",
      "epoch 26; iter: 0; batch adjuster loss: 0.053471; batch classifier loss; 0.430758; batch adversarial loss: 0.544115\n",
      "epoch 26; iter: 200; batch adjuster loss: 0.067121; batch classifier loss; 0.426078; batch adversarial loss: 0.591754\n",
      "epoch 27; iter: 0; batch adjuster loss: 0.087570; batch classifier loss; 0.438810; batch adversarial loss: 0.633359\n",
      "epoch 27; iter: 200; batch adjuster loss: 0.088753; batch classifier loss; 0.321191; batch adversarial loss: 0.663180\n",
      "epoch 28; iter: 0; batch adjuster loss: 0.064458; batch classifier loss; 0.510157; batch adversarial loss: 0.579893\n",
      "epoch 28; iter: 200; batch adjuster loss: 0.096036; batch classifier loss; 0.511230; batch adversarial loss: 0.674754\n",
      "epoch 29; iter: 0; batch adjuster loss: 0.054063; batch classifier loss; 0.392897; batch adversarial loss: 0.597059\n",
      "epoch 29; iter: 200; batch adjuster loss: 0.080737; batch classifier loss; 0.406677; batch adversarial loss: 0.547332\n",
      "epoch 30; iter: 0; batch adjuster loss: 0.098417; batch classifier loss; 0.377437; batch adversarial loss: 0.601598\n",
      "epoch 30; iter: 200; batch adjuster loss: 0.076913; batch classifier loss; 0.415697; batch adversarial loss: 0.573369\n",
      "epoch 31; iter: 0; batch adjuster loss: 0.083840; batch classifier loss; 0.445801; batch adversarial loss: 0.601289\n",
      "epoch 31; iter: 200; batch adjuster loss: 0.068736; batch classifier loss; 0.403146; batch adversarial loss: 0.571575\n",
      "epoch 32; iter: 0; batch adjuster loss: 0.109468; batch classifier loss; 0.442396; batch adversarial loss: 0.630340\n",
      "epoch 32; iter: 200; batch adjuster loss: 0.076064; batch classifier loss; 0.342722; batch adversarial loss: 0.566045\n",
      "epoch 33; iter: 0; batch adjuster loss: 0.092642; batch classifier loss; 0.477457; batch adversarial loss: 0.516118\n",
      "epoch 33; iter: 200; batch adjuster loss: 0.070909; batch classifier loss; 0.448660; batch adversarial loss: 0.562691\n",
      "epoch 34; iter: 0; batch adjuster loss: 0.104156; batch classifier loss; 0.428270; batch adversarial loss: 0.586558\n",
      "epoch 34; iter: 200; batch adjuster loss: 0.105042; batch classifier loss; 0.375315; batch adversarial loss: 0.606935\n",
      "epoch 35; iter: 0; batch adjuster loss: 0.074308; batch classifier loss; 0.349952; batch adversarial loss: 0.556847\n",
      "epoch 35; iter: 200; batch adjuster loss: 0.058704; batch classifier loss; 0.389458; batch adversarial loss: 0.577608\n",
      "epoch 36; iter: 0; batch adjuster loss: 0.066529; batch classifier loss; 0.410727; batch adversarial loss: 0.588462\n",
      "epoch 36; iter: 200; batch adjuster loss: 0.094344; batch classifier loss; 0.429880; batch adversarial loss: 0.582354\n",
      "epoch 37; iter: 0; batch adjuster loss: 0.088139; batch classifier loss; 0.450735; batch adversarial loss: 0.534966\n",
      "epoch 37; iter: 200; batch adjuster loss: 0.096504; batch classifier loss; 0.495152; batch adversarial loss: 0.617712\n",
      "epoch 38; iter: 0; batch adjuster loss: 0.109513; batch classifier loss; 0.367657; batch adversarial loss: 0.653136\n",
      "epoch 38; iter: 200; batch adjuster loss: 0.062211; batch classifier loss; 0.382535; batch adversarial loss: 0.590336\n",
      "epoch 39; iter: 0; batch adjuster loss: 0.058070; batch classifier loss; 0.444444; batch adversarial loss: 0.585441\n",
      "epoch 39; iter: 200; batch adjuster loss: 0.064483; batch classifier loss; 0.444134; batch adversarial loss: 0.547539\n",
      "epoch 40; iter: 0; batch adjuster loss: 0.054919; batch classifier loss; 0.423319; batch adversarial loss: 0.536894\n",
      "epoch 40; iter: 200; batch adjuster loss: 0.126867; batch classifier loss; 0.485131; batch adversarial loss: 0.567613\n",
      "epoch 41; iter: 0; batch adjuster loss: 0.101475; batch classifier loss; 0.483024; batch adversarial loss: 0.574811\n",
      "epoch 41; iter: 200; batch adjuster loss: 0.098981; batch classifier loss; 0.413305; batch adversarial loss: 0.590668\n",
      "epoch 42; iter: 0; batch adjuster loss: 0.087374; batch classifier loss; 0.368875; batch adversarial loss: 0.579498\n",
      "epoch 42; iter: 200; batch adjuster loss: 0.081320; batch classifier loss; 0.398704; batch adversarial loss: 0.603147\n",
      "epoch 43; iter: 0; batch adjuster loss: 0.085670; batch classifier loss; 0.456505; batch adversarial loss: 0.560857\n",
      "epoch 43; iter: 200; batch adjuster loss: 0.097122; batch classifier loss; 0.348219; batch adversarial loss: 0.580881\n",
      "epoch 44; iter: 0; batch adjuster loss: 0.089609; batch classifier loss; 0.391036; batch adversarial loss: 0.612436\n",
      "epoch 44; iter: 200; batch adjuster loss: 0.152566; batch classifier loss; 0.471999; batch adversarial loss: 0.656775\n",
      "epoch 45; iter: 0; batch adjuster loss: 0.124418; batch classifier loss; 0.408939; batch adversarial loss: 0.641658\n",
      "epoch 45; iter: 200; batch adjuster loss: 0.164816; batch classifier loss; 0.400181; batch adversarial loss: 0.634857\n",
      "epoch 46; iter: 0; batch adjuster loss: 0.094357; batch classifier loss; 0.475268; batch adversarial loss: 0.553455\n",
      "epoch 46; iter: 200; batch adjuster loss: 0.093982; batch classifier loss; 0.302012; batch adversarial loss: 0.580980\n",
      "epoch 47; iter: 0; batch adjuster loss: 0.126988; batch classifier loss; 0.400468; batch adversarial loss: 0.612188\n",
      "epoch 47; iter: 200; batch adjuster loss: 0.134834; batch classifier loss; 0.520254; batch adversarial loss: 0.622565\n",
      "epoch 48; iter: 0; batch adjuster loss: 0.140898; batch classifier loss; 0.368504; batch adversarial loss: 0.637091\n",
      "epoch 48; iter: 200; batch adjuster loss: 0.129809; batch classifier loss; 0.387143; batch adversarial loss: 0.601831\n",
      "epoch 49; iter: 0; batch adjuster loss: 0.117945; batch classifier loss; 0.437387; batch adversarial loss: 0.571720\n",
      "epoch 49; iter: 200; batch adjuster loss: 0.154933; batch classifier loss; 0.427756; batch adversarial loss: 0.611916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.fairness_adjuster.FairnessAdjuster at 0x7fafb00eb400>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debiased_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 18:50:33.237338: W tensorflow/c/c_api.cc:305] Operation '{name:'debiased_classifier/debiased_classifier/classifier_model/b2/Adam_1/Assign' id:268 op device:{requested: '', assigned: ''} def:{{{node debiased_classifier/debiased_classifier/classifier_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](debiased_classifier/debiased_classifier/classifier_model/b2/Adam_1, debiased_classifier/debiased_classifier/classifier_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_debiasing_train = debiased_model.predict(dataset_orig_train)\n",
    "dataset_debiasing_test = debiased_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.243550\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.238385\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Model - with debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.078153\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.072018\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.804067\n",
      "Test set: Balanced classification accuracy = 0.673824\n",
      "Test set: Disparate impact = 0.000000\n",
      "Test set: Equal opportunity difference = -0.502031\n",
      "Test set: Average odds difference = -0.313805\n",
      "Test set: Theil_index = 0.169696\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Model - with debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.791647\n",
      "Test set: Balanced classification accuracy = 0.670309\n",
      "Test set: Disparate impact = 0.644844\n",
      "Test set: Equal opportunity difference = -0.052855\n",
      "Test set: Average odds difference = -0.026681\n",
      "Test set: Theil_index = 0.170720\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "# Metrics for the dataset from model with debiasing\n",
    "display(Markdown(\"#### Model - with debiasing - dataset metrics\"))\n",
    "metric_dataset_debiasing_train = BinaryLabelDatasetMetric(dataset_debiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_debiasing_test = BinaryLabelDatasetMetric(dataset_debiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_test.mean_difference())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Model - with debiasing - classification metrics\"))\n",
    "classified_metric_debiasing_test = ClassificationMetric(dataset_orig_test, \n",
    "                                                 dataset_debiasing_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test.accuracy())\n",
    "TPR = classified_metric_debiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_debiasing_test.true_negative_rate()\n",
    "bal_acc_debiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    References:\n",
    "    [1] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating UnwantedBiases with Adversarial Learning,\" \n",
    "    AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aif360",
   "language": "python",
   "name": "aif360"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
