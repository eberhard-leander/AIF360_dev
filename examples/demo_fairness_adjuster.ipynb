{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates the use of the fairness adjuster, using the same structure as the AIF360 adversarial debiasing example\n",
    "\n",
    "The source notebook can be found here:\n",
    "https://github.com/Trusted-AI/AIF360/blob/main/examples/demo_adversarial_debiasing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 19:59:39.547270: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-09 19:59:39.549863: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-09 19:59:39.578970: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-09 19:59:39.578995: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-09 19:59:39.579018: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-09 19:59:39.585147: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-09 19:59:39.585945: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-09 19:59:40.203824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import (\n",
    "    load_preproc_data_adult,\n",
    "    load_preproc_data_compas,\n",
    "    load_preproc_data_german,\n",
    ")\n",
    "\n",
    "from aif360.algorithms.inprocessing.fairness_adjuster import FairnessAdjuster\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "SEED = 1\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "dataset_orig = load_preproc_data_adult()\n",
    "\n",
    "privileged_groups = [{\"sex\": 1}]\n",
    "unprivileged_groups = [{\"sex\": 0}]\n",
    "\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34189, 18)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex', 'race']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.])] [array([0.]), array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(\n",
    "    dataset_orig_train.privileged_protected_attributes,\n",
    "    dataset_orig_train.unprivileged_protected_attributes,\n",
    ")\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric for original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.190244\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.204482\n"
     ]
    }
   ],
   "source": [
    "# Metric for the original dataset\n",
    "metric_orig_train = BinaryLabelDatasetMetric(\n",
    "    dataset_orig_train,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\n",
    "    \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_orig_train.mean_difference()\n",
    ")\n",
    "metric_orig_test = BinaryLabelDatasetMetric(\n",
    "    dataset_orig_test,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "print(\n",
    "    \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_orig_test.mean_difference()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Scaled dataset - Verify that the scaling does not affect the group label statistics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.190244\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.204482\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MaxAbsScaler()\n",
    "dataset_orig_train.features = min_max_scaler.fit_transform(dataset_orig_train.features)\n",
    "dataset_orig_test.features = min_max_scaler.transform(dataset_orig_test.features)\n",
    "metric_scaled_train = BinaryLabelDatasetMetric(\n",
    "    dataset_orig_train,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "display(\n",
    "    Markdown(\n",
    "        \"#### Scaled dataset - Verify that the scaling does not affect the group label statistics\"\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_scaled_train.mean_difference()\n",
    ")\n",
    "metric_scaled_test = BinaryLabelDatasetMetric(\n",
    "    dataset_orig_test,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "print(\n",
    "    \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_scaled_test.mean_difference()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn plan classifier without debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load post-processing algorithm that equalizes the odds\n",
    "# Learn parameters with debias set to False\n",
    "sess = tf.Session()\n",
    "plain_model = FairnessAdjuster(\n",
    "    privileged_groups=privileged_groups,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    scope_name=\"plain_classifier\",\n",
    "    debias=False,\n",
    "    sess=sess,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/aif360/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 19:59:42.955843: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.777370\n",
      "epoch 0; iter: 200; batch classifier loss: 0.434442\n",
      "epoch 1; iter: 0; batch classifier loss: 0.404269\n",
      "epoch 1; iter: 200; batch classifier loss: 0.326910\n",
      "epoch 2; iter: 0; batch classifier loss: 0.350508\n",
      "epoch 2; iter: 200; batch classifier loss: 0.493786\n",
      "epoch 3; iter: 0; batch classifier loss: 0.519997\n",
      "epoch 3; iter: 200; batch classifier loss: 0.501568\n",
      "epoch 4; iter: 0; batch classifier loss: 0.467276\n",
      "epoch 4; iter: 200; batch classifier loss: 0.445991\n",
      "epoch 5; iter: 0; batch classifier loss: 0.375114\n",
      "epoch 5; iter: 200; batch classifier loss: 0.497320\n",
      "epoch 6; iter: 0; batch classifier loss: 0.462191\n",
      "epoch 6; iter: 200; batch classifier loss: 0.418625\n",
      "epoch 7; iter: 0; batch classifier loss: 0.434002\n",
      "epoch 7; iter: 200; batch classifier loss: 0.392433\n",
      "epoch 8; iter: 0; batch classifier loss: 0.401344\n",
      "epoch 8; iter: 200; batch classifier loss: 0.536455\n",
      "epoch 9; iter: 0; batch classifier loss: 0.369766\n",
      "epoch 9; iter: 200; batch classifier loss: 0.397298\n",
      "epoch 10; iter: 0; batch classifier loss: 0.423272\n",
      "epoch 10; iter: 200; batch classifier loss: 0.400359\n",
      "epoch 11; iter: 0; batch classifier loss: 0.420042\n",
      "epoch 11; iter: 200; batch classifier loss: 0.382492\n",
      "epoch 12; iter: 0; batch classifier loss: 0.391559\n",
      "epoch 12; iter: 200; batch classifier loss: 0.437075\n",
      "epoch 13; iter: 0; batch classifier loss: 0.481840\n",
      "epoch 13; iter: 200; batch classifier loss: 0.376736\n",
      "epoch 14; iter: 0; batch classifier loss: 0.399132\n",
      "epoch 14; iter: 200; batch classifier loss: 0.493754\n",
      "epoch 15; iter: 0; batch classifier loss: 0.414442\n",
      "epoch 15; iter: 200; batch classifier loss: 0.372358\n",
      "epoch 16; iter: 0; batch classifier loss: 0.312280\n",
      "epoch 16; iter: 200; batch classifier loss: 0.388971\n",
      "epoch 17; iter: 0; batch classifier loss: 0.460829\n",
      "epoch 17; iter: 200; batch classifier loss: 0.333290\n",
      "epoch 18; iter: 0; batch classifier loss: 0.420608\n",
      "epoch 18; iter: 200; batch classifier loss: 0.387492\n",
      "epoch 19; iter: 0; batch classifier loss: 0.485975\n",
      "epoch 19; iter: 200; batch classifier loss: 0.435154\n",
      "epoch 20; iter: 0; batch classifier loss: 0.380034\n",
      "epoch 20; iter: 200; batch classifier loss: 0.409072\n",
      "epoch 21; iter: 0; batch classifier loss: 0.458648\n",
      "epoch 21; iter: 200; batch classifier loss: 0.510061\n",
      "epoch 22; iter: 0; batch classifier loss: 0.510067\n",
      "epoch 22; iter: 200; batch classifier loss: 0.450371\n",
      "epoch 23; iter: 0; batch classifier loss: 0.405857\n",
      "epoch 23; iter: 200; batch classifier loss: 0.567712\n",
      "epoch 24; iter: 0; batch classifier loss: 0.368202\n",
      "epoch 24; iter: 200; batch classifier loss: 0.428764\n",
      "epoch 25; iter: 0; batch classifier loss: 0.338961\n",
      "epoch 25; iter: 200; batch classifier loss: 0.402864\n",
      "epoch 26; iter: 0; batch classifier loss: 0.415167\n",
      "epoch 26; iter: 200; batch classifier loss: 0.361658\n",
      "epoch 27; iter: 0; batch classifier loss: 0.507646\n",
      "epoch 27; iter: 200; batch classifier loss: 0.391080\n",
      "epoch 28; iter: 0; batch classifier loss: 0.413711\n",
      "epoch 28; iter: 200; batch classifier loss: 0.410405\n",
      "epoch 29; iter: 0; batch classifier loss: 0.429303\n",
      "epoch 29; iter: 200; batch classifier loss: 0.427015\n",
      "epoch 30; iter: 0; batch classifier loss: 0.567814\n",
      "epoch 30; iter: 200; batch classifier loss: 0.442714\n",
      "epoch 31; iter: 0; batch classifier loss: 0.436779\n",
      "epoch 31; iter: 200; batch classifier loss: 0.455926\n",
      "epoch 32; iter: 0; batch classifier loss: 0.386899\n",
      "epoch 32; iter: 200; batch classifier loss: 0.421569\n",
      "epoch 33; iter: 0; batch classifier loss: 0.401871\n",
      "epoch 33; iter: 200; batch classifier loss: 0.444027\n",
      "epoch 34; iter: 0; batch classifier loss: 0.460099\n",
      "epoch 34; iter: 200; batch classifier loss: 0.366095\n",
      "epoch 35; iter: 0; batch classifier loss: 0.398436\n",
      "epoch 35; iter: 200; batch classifier loss: 0.432692\n",
      "epoch 36; iter: 0; batch classifier loss: 0.432988\n",
      "epoch 36; iter: 200; batch classifier loss: 0.411751\n",
      "epoch 37; iter: 0; batch classifier loss: 0.479523\n",
      "epoch 37; iter: 200; batch classifier loss: 0.420060\n",
      "epoch 38; iter: 0; batch classifier loss: 0.424605\n",
      "epoch 38; iter: 200; batch classifier loss: 0.380397\n",
      "epoch 39; iter: 0; batch classifier loss: 0.497525\n",
      "epoch 39; iter: 200; batch classifier loss: 0.440861\n",
      "epoch 40; iter: 0; batch classifier loss: 0.421687\n",
      "epoch 40; iter: 200; batch classifier loss: 0.386183\n",
      "epoch 41; iter: 0; batch classifier loss: 0.465668\n",
      "epoch 41; iter: 200; batch classifier loss: 0.279313\n",
      "epoch 42; iter: 0; batch classifier loss: 0.397245\n",
      "epoch 42; iter: 200; batch classifier loss: 0.423027\n",
      "epoch 43; iter: 0; batch classifier loss: 0.462159\n",
      "epoch 43; iter: 200; batch classifier loss: 0.442286\n",
      "epoch 44; iter: 0; batch classifier loss: 0.463582\n",
      "epoch 44; iter: 200; batch classifier loss: 0.364366\n",
      "epoch 45; iter: 0; batch classifier loss: 0.409343\n",
      "epoch 45; iter: 200; batch classifier loss: 0.363781\n",
      "epoch 46; iter: 0; batch classifier loss: 0.398731\n",
      "epoch 46; iter: 200; batch classifier loss: 0.399638\n",
      "epoch 47; iter: 0; batch classifier loss: 0.384180\n",
      "epoch 47; iter: 200; batch classifier loss: 0.400598\n",
      "epoch 48; iter: 0; batch classifier loss: 0.414346\n",
      "epoch 48; iter: 200; batch classifier loss: 0.373357\n",
      "epoch 49; iter: 0; batch classifier loss: 0.391689\n",
      "epoch 49; iter: 200; batch classifier loss: 0.393186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.fairness_adjuster.FairnessAdjuster at 0x7f596c3b9d20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_nodebiasing_train = plain_model.predict(dataset_orig_train)\n",
    "dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.233568\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.239190\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.802703\n",
      "Test set: Balanced classification accuracy = 0.673437\n",
      "Test set: Disparate impact = 0.000000\n",
      "Test set: Equal opportunity difference = -0.494238\n",
      "Test set: Average odds difference = -0.309304\n",
      "Test set: Theil_index = 0.172950\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "metric_dataset_nodebiasing_train = BinaryLabelDatasetMetric(\n",
    "    dataset_nodebiasing_train,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_dataset_nodebiasing_train.mean_difference()\n",
    ")\n",
    "\n",
    "metric_dataset_nodebiasing_test = BinaryLabelDatasetMetric(\n",
    "    dataset_nodebiasing_test,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_dataset_nodebiasing_test.mean_difference()\n",
    ")\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "classified_metric_nodebiasing_test = ClassificationMetric(\n",
    "    dataset_orig_test,\n",
    "    dataset_nodebiasing_test,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "print(\n",
    "    \"Test set: Classification accuracy = %f\"\n",
    "    % classified_metric_nodebiasing_test.accuracy()\n",
    ")\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5 * (TPR + TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\n",
    "    \"Test set: Disparate impact = %f\"\n",
    "    % classified_metric_nodebiasing_test.disparate_impact()\n",
    ")\n",
    "print(\n",
    "    \"Test set: Equal opportunity difference = %f\"\n",
    "    % classified_metric_nodebiasing_test.equal_opportunity_difference()\n",
    ")\n",
    "print(\n",
    "    \"Test set: Average odds difference = %f\"\n",
    "    % classified_metric_nodebiasing_test.average_odds_difference()\n",
    ")\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply in-processing algorithm based on adversarial learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess2 = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn parameters with debias set to True\n",
    "debiased_model = FairnessAdjuster(\n",
    "    privileged_groups=privileged_groups,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    scope_name=\"debiased_classifier\",\n",
    "    adversary_loss_weight=0.1,\n",
    "    debias=True,\n",
    "    sess=sess2,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.777370\n",
      "epoch 0; iter: 200; batch classifier loss: 0.434442\n",
      "epoch 1; iter: 0; batch classifier loss: 0.404269\n",
      "epoch 1; iter: 200; batch classifier loss: 0.326910\n",
      "epoch 2; iter: 0; batch classifier loss: 0.350508\n",
      "epoch 2; iter: 200; batch classifier loss: 0.493786\n",
      "epoch 3; iter: 0; batch classifier loss: 0.519997\n",
      "epoch 3; iter: 200; batch classifier loss: 0.501568\n",
      "epoch 4; iter: 0; batch classifier loss: 0.467276\n",
      "epoch 4; iter: 200; batch classifier loss: 0.445991\n",
      "epoch 5; iter: 0; batch classifier loss: 0.375114\n",
      "epoch 5; iter: 200; batch classifier loss: 0.497320\n",
      "epoch 6; iter: 0; batch classifier loss: 0.462191\n",
      "epoch 6; iter: 200; batch classifier loss: 0.418625\n",
      "epoch 7; iter: 0; batch classifier loss: 0.434002\n",
      "epoch 7; iter: 200; batch classifier loss: 0.392433\n",
      "epoch 8; iter: 0; batch classifier loss: 0.401344\n",
      "epoch 8; iter: 200; batch classifier loss: 0.536455\n",
      "epoch 9; iter: 0; batch classifier loss: 0.369766\n",
      "epoch 9; iter: 200; batch classifier loss: 0.397298\n",
      "epoch 10; iter: 0; batch classifier loss: 0.423272\n",
      "epoch 10; iter: 200; batch classifier loss: 0.400359\n",
      "epoch 11; iter: 0; batch classifier loss: 0.420042\n",
      "epoch 11; iter: 200; batch classifier loss: 0.382492\n",
      "epoch 12; iter: 0; batch classifier loss: 0.391559\n",
      "epoch 12; iter: 200; batch classifier loss: 0.437075\n",
      "epoch 13; iter: 0; batch classifier loss: 0.481840\n",
      "epoch 13; iter: 200; batch classifier loss: 0.376736\n",
      "epoch 14; iter: 0; batch classifier loss: 0.399132\n",
      "epoch 14; iter: 200; batch classifier loss: 0.493754\n",
      "epoch 15; iter: 0; batch classifier loss: 0.414442\n",
      "epoch 15; iter: 200; batch classifier loss: 0.372358\n",
      "epoch 16; iter: 0; batch classifier loss: 0.312280\n",
      "epoch 16; iter: 200; batch classifier loss: 0.388971\n",
      "epoch 17; iter: 0; batch classifier loss: 0.460829\n",
      "epoch 17; iter: 200; batch classifier loss: 0.333290\n",
      "epoch 18; iter: 0; batch classifier loss: 0.420608\n",
      "epoch 18; iter: 200; batch classifier loss: 0.387492\n",
      "epoch 19; iter: 0; batch classifier loss: 0.485975\n",
      "epoch 19; iter: 200; batch classifier loss: 0.435154\n",
      "epoch 20; iter: 0; batch classifier loss: 0.380034\n",
      "epoch 20; iter: 200; batch classifier loss: 0.409072\n",
      "epoch 21; iter: 0; batch classifier loss: 0.458648\n",
      "epoch 21; iter: 200; batch classifier loss: 0.510061\n",
      "epoch 22; iter: 0; batch classifier loss: 0.510067\n",
      "epoch 22; iter: 200; batch classifier loss: 0.450371\n",
      "epoch 23; iter: 0; batch classifier loss: 0.405857\n",
      "epoch 23; iter: 200; batch classifier loss: 0.567712\n",
      "epoch 24; iter: 0; batch classifier loss: 0.368202\n",
      "epoch 24; iter: 200; batch classifier loss: 0.428764\n",
      "epoch 25; iter: 0; batch classifier loss: 0.338961\n",
      "epoch 25; iter: 200; batch classifier loss: 0.402864\n",
      "epoch 26; iter: 0; batch classifier loss: 0.415167\n",
      "epoch 26; iter: 200; batch classifier loss: 0.361658\n",
      "epoch 27; iter: 0; batch classifier loss: 0.507646\n",
      "epoch 27; iter: 200; batch classifier loss: 0.391080\n",
      "epoch 28; iter: 0; batch classifier loss: 0.413711\n",
      "epoch 28; iter: 200; batch classifier loss: 0.410405\n",
      "epoch 29; iter: 0; batch classifier loss: 0.429303\n",
      "epoch 29; iter: 200; batch classifier loss: 0.427015\n",
      "epoch 30; iter: 0; batch classifier loss: 0.567814\n",
      "epoch 30; iter: 200; batch classifier loss: 0.442714\n",
      "epoch 31; iter: 0; batch classifier loss: 0.436779\n",
      "epoch 31; iter: 200; batch classifier loss: 0.455926\n",
      "epoch 32; iter: 0; batch classifier loss: 0.386899\n",
      "epoch 32; iter: 200; batch classifier loss: 0.421569\n",
      "epoch 33; iter: 0; batch classifier loss: 0.401871\n",
      "epoch 33; iter: 200; batch classifier loss: 0.444027\n",
      "epoch 34; iter: 0; batch classifier loss: 0.460099\n",
      "epoch 34; iter: 200; batch classifier loss: 0.366095\n",
      "epoch 35; iter: 0; batch classifier loss: 0.398436\n",
      "epoch 35; iter: 200; batch classifier loss: 0.432692\n",
      "epoch 36; iter: 0; batch classifier loss: 0.432988\n",
      "epoch 36; iter: 200; batch classifier loss: 0.411751\n",
      "epoch 37; iter: 0; batch classifier loss: 0.479523\n",
      "epoch 37; iter: 200; batch classifier loss: 0.420060\n",
      "epoch 38; iter: 0; batch classifier loss: 0.424605\n",
      "epoch 38; iter: 200; batch classifier loss: 0.380397\n",
      "epoch 39; iter: 0; batch classifier loss: 0.497525\n",
      "epoch 39; iter: 200; batch classifier loss: 0.440861\n",
      "epoch 40; iter: 0; batch classifier loss: 0.421687\n",
      "epoch 40; iter: 200; batch classifier loss: 0.386183\n",
      "epoch 41; iter: 0; batch classifier loss: 0.465668\n",
      "epoch 41; iter: 200; batch classifier loss: 0.279313\n",
      "epoch 42; iter: 0; batch classifier loss: 0.397245\n",
      "epoch 42; iter: 200; batch classifier loss: 0.423027\n",
      "epoch 43; iter: 0; batch classifier loss: 0.462159\n",
      "epoch 43; iter: 200; batch classifier loss: 0.442286\n",
      "epoch 44; iter: 0; batch classifier loss: 0.463582\n",
      "epoch 44; iter: 200; batch classifier loss: 0.364366\n",
      "epoch 45; iter: 0; batch classifier loss: 0.409343\n",
      "epoch 45; iter: 200; batch classifier loss: 0.363781\n",
      "epoch 46; iter: 0; batch classifier loss: 0.398731\n",
      "epoch 46; iter: 200; batch classifier loss: 0.399638\n",
      "epoch 47; iter: 0; batch classifier loss: 0.384180\n",
      "epoch 47; iter: 200; batch classifier loss: 0.400598\n",
      "epoch 48; iter: 0; batch classifier loss: 0.414346\n",
      "epoch 48; iter: 200; batch classifier loss: 0.373357\n",
      "epoch 49; iter: 0; batch classifier loss: 0.391689\n",
      "epoch 49; iter: 200; batch classifier loss: 0.393186\n",
      "epoch 0; iter: 0; batch adjuster loss: 0.042378; batch classifier loss; 0.431486; batch adversarial loss: 0.569214\n",
      "epoch 0; iter: 200; batch adjuster loss: 0.058359; batch classifier loss; 0.387956; batch adversarial loss: 0.598599\n",
      "epoch 1; iter: 0; batch adjuster loss: 0.181015; batch classifier loss; 0.432170; batch adversarial loss: 0.649809\n",
      "epoch 1; iter: 200; batch adjuster loss: 0.771438; batch classifier loss; 0.438519; batch adversarial loss: 0.732767\n",
      "epoch 2; iter: 0; batch adjuster loss: 1.092041; batch classifier loss; 0.445192; batch adversarial loss: 0.743613\n",
      "epoch 2; iter: 200; batch adjuster loss: 1.290720; batch classifier loss; 0.566624; batch adversarial loss: 0.667907\n",
      "epoch 3; iter: 0; batch adjuster loss: 0.965922; batch classifier loss; 0.555481; batch adversarial loss: 0.658328\n",
      "epoch 3; iter: 200; batch adjuster loss: 0.232153; batch classifier loss; 0.527786; batch adversarial loss: 0.600818\n",
      "epoch 4; iter: 0; batch adjuster loss: 0.206568; batch classifier loss; 0.495708; batch adversarial loss: 0.622095\n",
      "epoch 4; iter: 200; batch adjuster loss: 0.062878; batch classifier loss; 0.446048; batch adversarial loss: 0.615057\n",
      "epoch 5; iter: 0; batch adjuster loss: 0.052628; batch classifier loss; 0.370958; batch adversarial loss: 0.605022\n",
      "epoch 5; iter: 200; batch adjuster loss: 0.075407; batch classifier loss; 0.517904; batch adversarial loss: 0.605326\n",
      "epoch 6; iter: 0; batch adjuster loss: 0.059043; batch classifier loss; 0.452468; batch adversarial loss: 0.604376\n",
      "epoch 6; iter: 200; batch adjuster loss: 0.104177; batch classifier loss; 0.424377; batch adversarial loss: 0.638925\n",
      "epoch 7; iter: 0; batch adjuster loss: 0.129699; batch classifier loss; 0.443329; batch adversarial loss: 0.626599\n",
      "epoch 7; iter: 200; batch adjuster loss: 0.192936; batch classifier loss; 0.437953; batch adversarial loss: 0.601674\n",
      "epoch 8; iter: 0; batch adjuster loss: 0.188444; batch classifier loss; 0.384844; batch adversarial loss: 0.666037\n",
      "epoch 8; iter: 200; batch adjuster loss: 0.160825; batch classifier loss; 0.562361; batch adversarial loss: 0.601399\n",
      "epoch 9; iter: 0; batch adjuster loss: 0.160378; batch classifier loss; 0.410430; batch adversarial loss: 0.618357\n",
      "epoch 9; iter: 200; batch adjuster loss: 0.197424; batch classifier loss; 0.418770; batch adversarial loss: 0.622278\n",
      "epoch 10; iter: 0; batch adjuster loss: 0.176573; batch classifier loss; 0.424005; batch adversarial loss: 0.617531\n",
      "epoch 10; iter: 200; batch adjuster loss: 0.145208; batch classifier loss; 0.410000; batch adversarial loss: 0.590659\n",
      "epoch 11; iter: 0; batch adjuster loss: 0.164406; batch classifier loss; 0.439554; batch adversarial loss: 0.588222\n",
      "epoch 11; iter: 200; batch adjuster loss: 0.212626; batch classifier loss; 0.371108; batch adversarial loss: 0.620736\n",
      "epoch 12; iter: 0; batch adjuster loss: 0.133950; batch classifier loss; 0.382597; batch adversarial loss: 0.582020\n",
      "epoch 12; iter: 200; batch adjuster loss: 0.187193; batch classifier loss; 0.462024; batch adversarial loss: 0.646224\n",
      "epoch 13; iter: 0; batch adjuster loss: 0.252406; batch classifier loss; 0.526219; batch adversarial loss: 0.643001\n",
      "epoch 13; iter: 200; batch adjuster loss: 0.157995; batch classifier loss; 0.355375; batch adversarial loss: 0.636728\n",
      "epoch 14; iter: 0; batch adjuster loss: 0.167708; batch classifier loss; 0.407499; batch adversarial loss: 0.594170\n",
      "epoch 14; iter: 200; batch adjuster loss: 0.169904; batch classifier loss; 0.488561; batch adversarial loss: 0.615807\n",
      "epoch 15; iter: 0; batch adjuster loss: 0.259654; batch classifier loss; 0.434155; batch adversarial loss: 0.665006\n",
      "epoch 15; iter: 200; batch adjuster loss: 0.166083; batch classifier loss; 0.389711; batch adversarial loss: 0.584630\n",
      "epoch 16; iter: 0; batch adjuster loss: 0.134520; batch classifier loss; 0.324899; batch adversarial loss: 0.579245\n",
      "epoch 16; iter: 200; batch adjuster loss: 0.199548; batch classifier loss; 0.401828; batch adversarial loss: 0.623068\n",
      "epoch 17; iter: 0; batch adjuster loss: 0.217352; batch classifier loss; 0.479422; batch adversarial loss: 0.642166\n",
      "epoch 17; iter: 200; batch adjuster loss: 0.106114; batch classifier loss; 0.330733; batch adversarial loss: 0.568088\n",
      "epoch 18; iter: 0; batch adjuster loss: 0.152637; batch classifier loss; 0.442193; batch adversarial loss: 0.581769\n",
      "epoch 18; iter: 200; batch adjuster loss: 0.119582; batch classifier loss; 0.402834; batch adversarial loss: 0.587061\n",
      "epoch 19; iter: 0; batch adjuster loss: 0.177111; batch classifier loss; 0.510940; batch adversarial loss: 0.564405\n",
      "epoch 19; iter: 200; batch adjuster loss: 0.183207; batch classifier loss; 0.452264; batch adversarial loss: 0.613392\n",
      "epoch 20; iter: 0; batch adjuster loss: 0.148386; batch classifier loss; 0.378625; batch adversarial loss: 0.597419\n",
      "epoch 20; iter: 200; batch adjuster loss: 0.269062; batch classifier loss; 0.437917; batch adversarial loss: 0.582662\n",
      "epoch 21; iter: 0; batch adjuster loss: 0.255056; batch classifier loss; 0.453431; batch adversarial loss: 0.638815\n",
      "epoch 21; iter: 200; batch adjuster loss: 0.137743; batch classifier loss; 0.524103; batch adversarial loss: 0.553412\n",
      "epoch 22; iter: 0; batch adjuster loss: 0.187602; batch classifier loss; 0.518981; batch adversarial loss: 0.605305\n",
      "epoch 22; iter: 200; batch adjuster loss: 0.146574; batch classifier loss; 0.470171; batch adversarial loss: 0.559749\n",
      "epoch 23; iter: 0; batch adjuster loss: 0.161879; batch classifier loss; 0.397504; batch adversarial loss: 0.658410\n",
      "epoch 23; iter: 200; batch adjuster loss: 0.258744; batch classifier loss; 0.546536; batch adversarial loss: 0.673943\n",
      "epoch 24; iter: 0; batch adjuster loss: 0.147591; batch classifier loss; 0.362487; batch adversarial loss: 0.583333\n",
      "epoch 24; iter: 200; batch adjuster loss: 0.190606; batch classifier loss; 0.451787; batch adversarial loss: 0.559212\n",
      "epoch 25; iter: 0; batch adjuster loss: 0.160570; batch classifier loss; 0.363534; batch adversarial loss: 0.581171\n",
      "epoch 25; iter: 200; batch adjuster loss: 0.221572; batch classifier loss; 0.443831; batch adversarial loss: 0.593650\n",
      "epoch 26; iter: 0; batch adjuster loss: 0.173159; batch classifier loss; 0.419221; batch adversarial loss: 0.611897\n",
      "epoch 26; iter: 200; batch adjuster loss: 0.173422; batch classifier loss; 0.374420; batch adversarial loss: 0.629332\n",
      "epoch 27; iter: 0; batch adjuster loss: 0.138559; batch classifier loss; 0.512506; batch adversarial loss: 0.585294\n",
      "epoch 27; iter: 200; batch adjuster loss: 0.191054; batch classifier loss; 0.408724; batch adversarial loss: 0.568442\n",
      "epoch 28; iter: 0; batch adjuster loss: 0.181062; batch classifier loss; 0.423433; batch adversarial loss: 0.586884\n",
      "epoch 28; iter: 200; batch adjuster loss: 0.204345; batch classifier loss; 0.427494; batch adversarial loss: 0.606770\n",
      "epoch 29; iter: 0; batch adjuster loss: 0.229673; batch classifier loss; 0.429233; batch adversarial loss: 0.634719\n",
      "epoch 29; iter: 200; batch adjuster loss: 0.253242; batch classifier loss; 0.421308; batch adversarial loss: 0.663805\n",
      "epoch 30; iter: 0; batch adjuster loss: 0.157930; batch classifier loss; 0.572256; batch adversarial loss: 0.598518\n",
      "epoch 30; iter: 200; batch adjuster loss: 0.255159; batch classifier loss; 0.462327; batch adversarial loss: 0.593402\n",
      "epoch 31; iter: 0; batch adjuster loss: 0.270470; batch classifier loss; 0.467558; batch adversarial loss: 0.607871\n",
      "epoch 31; iter: 200; batch adjuster loss: 0.253236; batch classifier loss; 0.470585; batch adversarial loss: 0.653977\n",
      "epoch 32; iter: 0; batch adjuster loss: 0.227908; batch classifier loss; 0.427197; batch adversarial loss: 0.555747\n",
      "epoch 32; iter: 200; batch adjuster loss: 0.210467; batch classifier loss; 0.413802; batch adversarial loss: 0.637601\n",
      "epoch 33; iter: 0; batch adjuster loss: 0.230739; batch classifier loss; 0.410940; batch adversarial loss: 0.586363\n",
      "epoch 33; iter: 200; batch adjuster loss: 0.197637; batch classifier loss; 0.471809; batch adversarial loss: 0.562791\n",
      "epoch 34; iter: 0; batch adjuster loss: 0.224379; batch classifier loss; 0.475961; batch adversarial loss: 0.606337\n",
      "epoch 34; iter: 200; batch adjuster loss: 0.239624; batch classifier loss; 0.367925; batch adversarial loss: 0.654967\n",
      "epoch 35; iter: 0; batch adjuster loss: 0.307965; batch classifier loss; 0.436549; batch adversarial loss: 0.644019\n",
      "epoch 35; iter: 200; batch adjuster loss: 0.204390; batch classifier loss; 0.408035; batch adversarial loss: 0.586878\n",
      "epoch 36; iter: 0; batch adjuster loss: 0.338957; batch classifier loss; 0.449760; batch adversarial loss: 0.669386\n",
      "epoch 36; iter: 200; batch adjuster loss: 0.212064; batch classifier loss; 0.431115; batch adversarial loss: 0.652809\n",
      "epoch 37; iter: 0; batch adjuster loss: 0.284781; batch classifier loss; 0.459345; batch adversarial loss: 0.691423\n",
      "epoch 37; iter: 200; batch adjuster loss: 0.256023; batch classifier loss; 0.451380; batch adversarial loss: 0.604989\n",
      "epoch 38; iter: 0; batch adjuster loss: 0.258576; batch classifier loss; 0.433620; batch adversarial loss: 0.649007\n",
      "epoch 38; iter: 200; batch adjuster loss: 0.214501; batch classifier loss; 0.408443; batch adversarial loss: 0.597603\n",
      "epoch 39; iter: 0; batch adjuster loss: 0.212032; batch classifier loss; 0.508510; batch adversarial loss: 0.588112\n",
      "epoch 39; iter: 200; batch adjuster loss: 0.161966; batch classifier loss; 0.437090; batch adversarial loss: 0.578954\n",
      "epoch 40; iter: 0; batch adjuster loss: 0.246269; batch classifier loss; 0.428485; batch adversarial loss: 0.621432\n",
      "epoch 40; iter: 200; batch adjuster loss: 0.256088; batch classifier loss; 0.411878; batch adversarial loss: 0.639661\n",
      "epoch 41; iter: 0; batch adjuster loss: 0.234859; batch classifier loss; 0.475475; batch adversarial loss: 0.575567\n",
      "epoch 41; iter: 200; batch adjuster loss: 0.210977; batch classifier loss; 0.295292; batch adversarial loss: 0.588889\n",
      "epoch 42; iter: 0; batch adjuster loss: 0.296475; batch classifier loss; 0.410616; batch adversarial loss: 0.605780\n",
      "epoch 42; iter: 200; batch adjuster loss: 0.233708; batch classifier loss; 0.463793; batch adversarial loss: 0.600898\n",
      "epoch 43; iter: 0; batch adjuster loss: 0.241519; batch classifier loss; 0.458316; batch adversarial loss: 0.628189\n",
      "epoch 43; iter: 200; batch adjuster loss: 0.234463; batch classifier loss; 0.465406; batch adversarial loss: 0.615344\n",
      "epoch 44; iter: 0; batch adjuster loss: 0.220069; batch classifier loss; 0.509291; batch adversarial loss: 0.544508\n",
      "epoch 44; iter: 200; batch adjuster loss: 0.193052; batch classifier loss; 0.376417; batch adversarial loss: 0.580576\n",
      "epoch 45; iter: 0; batch adjuster loss: 0.311152; batch classifier loss; 0.466763; batch adversarial loss: 0.599399\n",
      "epoch 45; iter: 200; batch adjuster loss: 0.247457; batch classifier loss; 0.382077; batch adversarial loss: 0.662170\n",
      "epoch 46; iter: 0; batch adjuster loss: 0.219664; batch classifier loss; 0.417550; batch adversarial loss: 0.560761\n",
      "epoch 46; iter: 200; batch adjuster loss: 0.305691; batch classifier loss; 0.394430; batch adversarial loss: 0.655189\n",
      "epoch 47; iter: 0; batch adjuster loss: 0.269146; batch classifier loss; 0.430777; batch adversarial loss: 0.588992\n",
      "epoch 47; iter: 200; batch adjuster loss: 0.339752; batch classifier loss; 0.451636; batch adversarial loss: 0.600897\n",
      "epoch 48; iter: 0; batch adjuster loss: 0.201172; batch classifier loss; 0.401003; batch adversarial loss: 0.610530\n",
      "epoch 48; iter: 200; batch adjuster loss: 0.257316; batch classifier loss; 0.385089; batch adversarial loss: 0.656625\n",
      "epoch 49; iter: 0; batch adjuster loss: 0.209481; batch classifier loss; 0.391560; batch adversarial loss: 0.575505\n",
      "epoch 49; iter: 200; batch adjuster loss: 0.249854; batch classifier loss; 0.415803; batch adversarial loss: 0.606167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.fairness_adjuster.FairnessAdjuster at 0x7f56fa186bf0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debiased_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 20:00:18.397390: W tensorflow/c/c_api.cc:305] Operation '{name:'debiased_classifier/debiased_classifier/classifier_model/b2/Adam_1/Assign' id:268 op device:{requested: '', assigned: ''} def:{{{node debiased_classifier/debiased_classifier/classifier_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](debiased_classifier/debiased_classifier/classifier_model/b2/Adam_1, debiased_classifier/debiased_classifier/classifier_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_debiasing_train = debiased_model.predict(dataset_orig_train)\n",
    "dataset_debiasing_test = debiased_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.233568\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.239190\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Model - with debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.064935\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.069512\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.802703\n",
      "Test set: Balanced classification accuracy = 0.673437\n",
      "Test set: Disparate impact = 0.000000\n",
      "Test set: Equal opportunity difference = -0.494238\n",
      "Test set: Average odds difference = -0.309304\n",
      "Test set: Theil_index = 0.172950\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Model - with debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.786460\n",
      "Test set: Balanced classification accuracy = 0.669223\n",
      "Test set: Disparate impact = 0.666324\n",
      "Test set: Equal opportunity difference = -0.008763\n",
      "Test set: Average odds difference = -0.003839\n",
      "Test set: Theil_index = 0.173868\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "print(\n",
    "    \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_dataset_nodebiasing_train.mean_difference()\n",
    ")\n",
    "print(\n",
    "    \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_dataset_nodebiasing_test.mean_difference()\n",
    ")\n",
    "\n",
    "# Metrics for the dataset from model with debiasing\n",
    "display(Markdown(\"#### Model - with debiasing - dataset metrics\"))\n",
    "metric_dataset_debiasing_train = BinaryLabelDatasetMetric(\n",
    "    dataset_debiasing_train,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_dataset_debiasing_train.mean_difference()\n",
    ")\n",
    "\n",
    "metric_dataset_debiasing_test = BinaryLabelDatasetMetric(\n",
    "    dataset_debiasing_test,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_dataset_debiasing_test.mean_difference()\n",
    ")\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "print(\n",
    "    \"Test set: Classification accuracy = %f\"\n",
    "    % classified_metric_nodebiasing_test.accuracy()\n",
    ")\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5 * (TPR + TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\n",
    "    \"Test set: Disparate impact = %f\"\n",
    "    % classified_metric_nodebiasing_test.disparate_impact()\n",
    ")\n",
    "print(\n",
    "    \"Test set: Equal opportunity difference = %f\"\n",
    "    % classified_metric_nodebiasing_test.equal_opportunity_difference()\n",
    ")\n",
    "print(\n",
    "    \"Test set: Average odds difference = %f\"\n",
    "    % classified_metric_nodebiasing_test.average_odds_difference()\n",
    ")\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Model - with debiasing - classification metrics\"))\n",
    "classified_metric_debiasing_test = ClassificationMetric(\n",
    "    dataset_orig_test,\n",
    "    dataset_debiasing_test,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "print(\n",
    "    \"Test set: Classification accuracy = %f\"\n",
    "    % classified_metric_debiasing_test.accuracy()\n",
    ")\n",
    "TPR = classified_metric_debiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_debiasing_test.true_negative_rate()\n",
    "bal_acc_debiasing_test = 0.5 * (TPR + TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test)\n",
    "print(\n",
    "    \"Test set: Disparate impact = %f\"\n",
    "    % classified_metric_debiasing_test.disparate_impact()\n",
    ")\n",
    "print(\n",
    "    \"Test set: Equal opportunity difference = %f\"\n",
    "    % classified_metric_debiasing_test.equal_opportunity_difference()\n",
    ")\n",
    "print(\n",
    "    \"Test set: Average odds difference = %f\"\n",
    "    % classified_metric_debiasing_test.average_odds_difference()\n",
    ")\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    References:\n",
    "    [1] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating UnwantedBiases with Adversarial Learning,\" \n",
    "    AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print predicted labels for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    11939\n",
       "1.0     2714\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(dataset_debiasing_test.labels.reshape(-1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    12313\n",
       "1.0     2340\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(dataset_nodebiasing_test.labels.reshape(-1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aif360",
   "language": "python",
   "name": "aif360"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
