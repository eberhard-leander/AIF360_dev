{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c1ceb52-54f2-4ec2-84b6-0f957a821a61",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates the use of adversarial debiasing algorithm to learn a fair classifier.\n",
    "Adversarial debiasing [1] is an in-processing technique that learns a classifier to maximize prediction accuracy and simultaneously reduce an adversary's ability to determine the protected attribute from the predictions. This approach leads to a fair classifier as the predictions cannot carry any group discrimination information that the adversary can exploit. We will see how to use this algorithm for learning models with and without fairness constraints and apply them on the Adult dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9880e0d5-9cbd-4b8e-aa72-712efcfb25ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 00:02:40.715146: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-17 00:02:40.727266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734393760.741142   58895 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734393760.745317   58895 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-17 00:02:40.759597: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ubuntu/miniconda3/envs/aif360/lib/python3.11/site-packages/inFairness/utils/ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "/home/ubuntu/miniconda3/envs/aif360/lib/python3.11/site-packages/inFairness/utils/ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n",
      "/home/ubuntu/miniconda3/envs/aif360/lib/python3.11/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from aif360.algorithms.inprocessing.xgb_adversarial_debiasing import (\n",
    "    XGBAdversarialDebiasing,\n",
    ")\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import (\n",
    "    load_preproc_data_german,\n",
    ")\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f4683b-2336-4563-a90e-ee7dae7b6fe2",
   "metadata": {},
   "source": [
    "#### Load dataset and set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4fe68c4-6f1b-455a-850d-507c53718838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AIF360_dev/aif360/algorithms/preprocessing/optim_preproc_helpers/data_preproc_functions.py:261: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['sex'] = df['personal_status'].replace(status_map)\n"
     ]
    }
   ],
   "source": [
    "dataset_orig = load_preproc_data_german([\"age\", \"sex\"])\n",
    "privileged_groups = [{\"age\": 1}]\n",
    "unprivileged_groups = [{\"age\": 0}]\n",
    "\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split(\n",
    "    [0.7], shuffle=True, seed=1234\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ed3b31e-b6b3-46b9-93d8-d82b05bbf8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 11)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.])] [array([0.]), array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'credit_history=Delay', 'credit_history=None/Paid', 'credit_history=Other', 'savings=500+', 'savings=<500', 'savings=Unknown/None', 'employment=1-4 years', 'employment=4+ years', 'employment=Unemployed']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(\n",
    "    dataset_orig_train.privileged_protected_attributes,\n",
    "    dataset_orig_train.unprivileged_protected_attributes,\n",
    ")\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ec93b-140e-4f66-af88-005e8fb72dcb",
   "metadata": {},
   "source": [
    "#### Metric for original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a4bb1fb-68ec-4045-99b2-9dc84b77191f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: Difference in mean outcomes between unprivileged and privileged groups = -0.149448\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.135639\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.183895\n"
     ]
    }
   ],
   "source": [
    "# Metric for the original dataset\n",
    "metric_orig = BinaryLabelDatasetMetric(\n",
    "    dataset_orig,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "\n",
    "display(Markdown(\"#### Original dataset\"))\n",
    "print(\n",
    "    \"Overall: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_orig.mean_difference()\n",
    ")\n",
    "# Metric for the training dataset\n",
    "\n",
    "metric_orig_train = BinaryLabelDatasetMetric(\n",
    "    dataset_orig_train,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\n",
    "    \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_orig_train.mean_difference()\n",
    ")\n",
    "# Metric for the Test dataset\n",
    "\n",
    "metric_orig_test = BinaryLabelDatasetMetric(\n",
    "    dataset_orig_test,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "print(\n",
    "    \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_orig_test.mean_difference()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a16b86d-e757-437c-9a81-7d19b946227e",
   "metadata": {},
   "source": [
    "### Learn plan classifier without debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e631abf2-4478-44af-8d3c-1b295446e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_model = XGBAdversarialDebiasing(\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    "    seed=1234,\n",
    "    debias=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e77a704-19c3-4371-affb-9733bf7f0278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.xgb_adversarial_debiasing.XGBAdversarialDebiasing at 0x7f2b98a13fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0491160-1a05-44c2-8810-09be145ec168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_nodebiasing_train = plain_model.predict(dataset_orig_train)\n",
    "dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ccfbab8-b16a-43ce-9ce5-218cdaaf4de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.246799\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.323342\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.723333\n",
      "Test set: Balanced classification accuracy = 0.535684\n",
      "Test set: Disparate impact = 0.669749\n",
      "Test set: Equal opportunity difference = -0.283934\n",
      "Test set: Average odds difference = -0.332708\n",
      "Test set: Theil_index = 0.104363\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "metric_dataset_nodebiasing_train = BinaryLabelDatasetMetric(\n",
    "    dataset_nodebiasing_train,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_dataset_nodebiasing_train.mean_difference()\n",
    ")\n",
    "\n",
    "metric_dataset_nodebiasing_test = BinaryLabelDatasetMetric(\n",
    "    dataset_nodebiasing_test,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_dataset_nodebiasing_test.mean_difference()\n",
    ")\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "classified_metric_nodebiasing_test = ClassificationMetric(\n",
    "    dataset_orig_test,\n",
    "    dataset_nodebiasing_test,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "print(\n",
    "    \"Test set: Classification accuracy = %f\"\n",
    "    % classified_metric_nodebiasing_test.accuracy()\n",
    ")\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5 * (TPR + TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\n",
    "    \"Test set: Disparate impact = %f\"\n",
    "    % classified_metric_nodebiasing_test.disparate_impact()\n",
    ")\n",
    "print(\n",
    "    \"Test set: Equal opportunity difference = %f\"\n",
    "    % classified_metric_nodebiasing_test.equal_opportunity_difference()\n",
    ")\n",
    "print(\n",
    "    \"Test set: Average odds difference = %f\"\n",
    "    % classified_metric_nodebiasing_test.average_odds_difference()\n",
    ")\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fc8347-736a-4e96-8cd3-c24d718cbdcd",
   "metadata": {},
   "source": [
    "### Apply in-processing algorithm based on adversarial learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a58c98dd-d323-4db3-98b7-2a94940721e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute_name = \"age\"\n",
    "Z = dataset_orig_train.protected_attributes[\n",
    "    :,\n",
    "    dataset_orig_train.protected_attribute_names.index(protected_attribute_name),\n",
    "]\n",
    "debiased_model = XGBAdversarialDebiasing(\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    "    seed=1234,\n",
    "    debias=True,\n",
    "    protected_group_vector=Z,\n",
    "    adversary_loss_weight=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fbf48ef-0b64-4fb7-90c2-34128da3be02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.xgb_adversarial_debiasing.XGBAdversarialDebiasing at 0x7f2b99629950>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debiased_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74db4876-6c63-4d95-aaaf-45bbd72f8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_debiasing_train = debiased_model.predict(dataset_orig_train)\n",
    "dataset_debiasing_test = debiased_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "038c9554-70a0-4d26-99be-29a242202fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.246799\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.323342\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Model - with debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.122741\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.265450\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.723333\n",
      "Test set: Balanced classification accuracy = 0.535684\n",
      "Test set: Disparate impact = 0.669749\n",
      "Test set: Equal opportunity difference = -0.283934\n",
      "Test set: Average odds difference = -0.332708\n",
      "Test set: Theil_index = 0.104363\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Model - with debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.716667\n",
      "Test set: Balanced classification accuracy = 0.535225\n",
      "Test set: Disparate impact = 0.721743\n",
      "Test set: Equal opportunity difference = -0.201351\n",
      "Test set: Average odds difference = -0.282157\n",
      "Test set: Theil_index = 0.114847\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "print(\n",
    "    \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_dataset_nodebiasing_train.mean_difference()\n",
    ")\n",
    "print(\n",
    "    \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_dataset_nodebiasing_test.mean_difference()\n",
    ")\n",
    "\n",
    "# Metrics for the dataset from model with debiasing\n",
    "display(Markdown(\"#### Model - with debiasing - dataset metrics\"))\n",
    "metric_dataset_debiasing_train = BinaryLabelDatasetMetric(\n",
    "    dataset_debiasing_train,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_dataset_debiasing_train.mean_difference()\n",
    ")\n",
    "\n",
    "metric_dataset_debiasing_test = BinaryLabelDatasetMetric(\n",
    "    dataset_debiasing_test,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "    % metric_dataset_debiasing_test.mean_difference()\n",
    ")\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "print(\n",
    "    \"Test set: Classification accuracy = %f\"\n",
    "    % classified_metric_nodebiasing_test.accuracy()\n",
    ")\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5 * (TPR + TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\n",
    "    \"Test set: Disparate impact = %f\"\n",
    "    % classified_metric_nodebiasing_test.disparate_impact()\n",
    ")\n",
    "print(\n",
    "    \"Test set: Equal opportunity difference = %f\"\n",
    "    % classified_metric_nodebiasing_test.equal_opportunity_difference()\n",
    ")\n",
    "print(\n",
    "    \"Test set: Average odds difference = %f\"\n",
    "    % classified_metric_nodebiasing_test.average_odds_difference()\n",
    ")\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Model - with debiasing - classification metrics\"))\n",
    "classified_metric_debiasing_test = ClassificationMetric(\n",
    "    dataset_orig_test,\n",
    "    dataset_debiasing_test,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    ")\n",
    "print(\n",
    "    \"Test set: Classification accuracy = %f\"\n",
    "    % classified_metric_debiasing_test.accuracy()\n",
    ")\n",
    "TPR = classified_metric_debiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_debiasing_test.true_negative_rate()\n",
    "bal_acc_debiasing_test = 0.5 * (TPR + TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test)\n",
    "print(\n",
    "    \"Test set: Disparate impact = %f\"\n",
    "    % classified_metric_debiasing_test.disparate_impact()\n",
    ")\n",
    "print(\n",
    "    \"Test set: Equal opportunity difference = %f\"\n",
    "    % classified_metric_debiasing_test.equal_opportunity_difference()\n",
    ")\n",
    "print(\n",
    "    \"Test set: Average odds difference = %f\"\n",
    "    % classified_metric_debiasing_test.average_odds_difference()\n",
    ")\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8892995-7732-4b40-9905-c8fc49194798",
   "metadata": {},
   "source": [
    "\n",
    "    References:\n",
    "    [1] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating UnwantedBiases with Adversarial Learning,\" \n",
    "    AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919c0be-354b-4d75-969b-160bd3e5184e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aif360",
   "language": "python",
   "name": "aif360"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
